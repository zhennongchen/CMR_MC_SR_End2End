{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this script includes: \n",
    "\n",
    "(1) turn the manual contours from control points to masks\n",
    "\n",
    "(2) only keep the slices that have segmentation (LV + myo)\n",
    "\n",
    "(3) n4 bias correction\n",
    "\n",
    "(4) min-max normalization\n",
    "\n",
    "(5) keep the voxel dimension uniform [1.25, 1.25, 2]\n",
    "\n",
    "(6) make ROI image for LV and myocardium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'CMR_HFpEF_Analysis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/workspace/Documents/CMR_HFpEF_Analysis/HFpEF_data_analysis/main_img_processing.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f47505534227d/workspace/Documents/CMR_HFpEF_Analysis/HFpEF_data_analysis/main_img_processing.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mCMR_HFpEF_Analysis\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mDefaults\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mDefaults\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f47505534227d/workspace/Documents/CMR_HFpEF_Analysis/HFpEF_data_analysis/main_img_processing.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mCMR_HFpEF_Analysis\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctions_collection\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mff\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f47505534227d/workspace/Documents/CMR_HFpEF_Analysis/HFpEF_data_analysis/main_img_processing.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mCMR_HFpEF_Analysis\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mImage_utils\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mutil\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'CMR_HFpEF_Analysis'"
     ]
    }
   ],
   "source": [
    "import CMR_HFpEF_Analysis.Defaults as Defaults\n",
    "import CMR_HFpEF_Analysis.functions_collection as ff\n",
    "import CMR_HFpEF_Analysis.Image_utils as util\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import SimpleITK as sitk\n",
    "import pandas as pd\n",
    "# import cv2\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "\n",
    "main_path = '/mnt/mount_zc_NAS/HFpEF/data/HFpEF_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main script (the product will be used in radiomics as the next step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main script\n",
    "case_list = ff.find_all_target_files(['ID_*'], os.path.join(main_path, 'contours'))\n",
    "\n",
    "for i in range(0,len(case_list)):\n",
    "    case = case_list[i]\n",
    "    case_id = os.path.basename(case)\n",
    "    print(i, case_id)\n",
    "\n",
    "    save_folder = os.path.join(main_path, 'masks', case_id)\n",
    "    if os.path.isfile(os.path.join(save_folder, 'mask_myo.nii.gz')) == 1:\n",
    "        print('done, continue')\n",
    "        continue\n",
    "\n",
    "    # find the ED \n",
    "    edes_file = os.path.join(main_path, 'ED_ES', case_id,'ED_ES.txt')\n",
    "    # prepare a blank txt:\n",
    "    with open(edes_file, 'r') as file:\n",
    "        line = file.readline().strip()\n",
    "        numbers = line.split()\n",
    "        ed = int(numbers[0])\n",
    "    print(ed)\n",
    "    # load the ED image\n",
    "    img_file = os.path.join(main_path, 'raw_img', case_id, 'Org3D_frame'+str(ed) +'.nrrd')\n",
    "    img_file = sitk.ReadImage(img_file)\n",
    "    spacing = img_file.GetSpacing()\n",
    "    print(spacing)\n",
    "    img = np.rollaxis(sitk.GetArrayFromImage(img_file),0,3)\n",
    "\n",
    "    # load contour points and turn into masks\n",
    "    # contour folder\n",
    "    contour_folder = os.path.join(main_path, 'contours',  case_id, 'manual_contours')\n",
    "    endo_files = ff.sort_timeframe(ff.find_all_target_files(['Endo*'],contour_folder),2,'_')\n",
    "    epi_files = ff.sort_timeframe(ff.find_all_target_files(['Epi*'],contour_folder),2,'_')\n",
    "    assert len(endo_files) == len(epi_files)\n",
    "\n",
    "    # convert to mask\n",
    "    seg = np.zeros(img.shape)\n",
    "\n",
    "    for file_i in range(0,len(endo_files)):\n",
    "        slice_index = ff.find_timeframe(endo_files[file_i], 2, '_')\n",
    "            \n",
    "        endo_pts = []; epi_pts = []\n",
    "\n",
    "        # get endo and epi contour points\n",
    "        with open(endo_files[file_i], 'r') as f:\n",
    "            endo_data = json.load(f)\n",
    "            endo_data = endo_data['markups'][0]['controlPoints']\n",
    "        for l in range(0,len(endo_data)):\n",
    "            # if it's manual drawing from 3D slicer, need to convert from mm to pixel unit\n",
    "            endo_pts.append([endo_data[l]['position'][0] / spacing[0], endo_data[l]['position'][1] / spacing[1]])\n",
    "\n",
    "        with open(epi_files[file_i], 'r') as f:\n",
    "            epi_data = json.load(f)\n",
    "        epi_data = epi_data['markups'][0]['controlPoints']\n",
    "        for l in range(0,len(epi_data)):\n",
    "            # if it's manual drawing from 3D slicer, need to convert from mm to pixel unit\n",
    "            epi_pts.append([epi_data[l]['position'][0] / spacing[0], epi_data[l]['position'][1] / spacing[1]])\n",
    "\n",
    "        cts_dict = {\"Endo\":np.asarray(endo_pts).astype(int), \"Epi\": np.asarray(epi_pts).astype(int), \"RV\": None}\n",
    "\n",
    "        seg[:,:,slice_index] = util.contourpts_to_mask(cts_dict, np.zeros([img.shape[0], img.shape[1]]),  ['LV', 'Myo'], [1,2], sample_rate = 1)\n",
    "    \n",
    "    # do some image processing\n",
    "    # only keep the slices that have segmentation\n",
    "    non_zero_slices = [z for z in range(seg.shape[2]) if np.sum(seg[:, :, z]) != 0]\n",
    "    img_heart = img[:,:, non_zero_slices]\n",
    "    seg_heart = seg[:,:,non_zero_slices]\n",
    "\n",
    "    # n4 bias correction\n",
    "    I = sitk.GetImageFromArray(img_heart)\n",
    "    I = sitk.Cast(I, sitk.sitkFloat32)\n",
    "    I = sitk.N4BiasFieldCorrection(I)\n",
    "    img_bias_corrected = sitk.GetArrayFromImage(I)\n",
    "\n",
    "    # make the min_max normalization into [0,255]\n",
    "    img_norm = ((img_bias_corrected - np.min(img_bias_corrected)) / (np.max(img_bias_corrected) - np.min(img_bias_corrected))) * 255\n",
    "\n",
    "    # make the voxel dimension uniform [1.25, 1.25, 2.0]\n",
    "    new_voxel_dim = [1.25,1.25,2]\n",
    "\n",
    "    zoom_factors = [original_dim / new_dim for new_dim, original_dim in zip(new_voxel_dim, spacing)]\n",
    "    img_zoom = zoom(img_norm, zoom_factors, order = 3)  # Order=3 for cubic intenrpolation\n",
    "    img_zoom = np.round(img_zoom)\n",
    "    seg_zoom = zoom(seg_heart, zoom_factors, order = 0) # Order = 0 for nearest neighbour\n",
    "    seg_zoom = np.round(seg_zoom)\n",
    "\n",
    "    seg_lv_zoom = np.copy(seg_zoom); seg_lv_zoom[seg_lv_zoom != 1] = 0\n",
    "    seg_myo_zoom = np.copy(seg_zoom); seg_myo_zoom[seg_myo_zoom != 2] = 0; seg_myo_zoom[seg_myo_zoom == 2] = 1\n",
    "\n",
    "    ff.make_folder([save_folder])\n",
    "    ff.save_itk(np.rollaxis(img_zoom,2,0), os.path.join(save_folder, 'img.nii.gz'), img_file, new_voxel_dim, np.eye(4))\n",
    "    ff.save_itk(np.rollaxis(seg_zoom,2,0), os.path.join(save_folder, 'mask.nii.gz'), img_file, new_voxel_dim, np.eye(4))\n",
    "    ff.save_itk(np.rollaxis(seg_lv_zoom,2,0), os.path.join(save_folder, 'mask_lv.nii.gz'), img_file, new_voxel_dim, np.eye(4))\n",
    "    ff.save_itk(np.rollaxis(seg_myo_zoom,2,0), os.path.join(save_folder, 'mask_myo.nii.gz'), img_file, new_voxel_dim, np.eye(4))\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just to process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID_0011 11\n",
      "(0.78125, 0.78125, 8.0)\n"
     ]
    }
   ],
   "source": [
    "# main script\n",
    "case_list = ff.find_all_target_files(['ID_*'], os.path.join(main_path, 'unchecked/contours'))\n",
    "\n",
    "for i in range(0,len(case_list)):\n",
    "    case = case_list[i]\n",
    "    case_id = os.path.basename(case)\n",
    "    case_id_num = ff.ID_00XX_to_XX(case_id)\n",
    "    print(case_id,case_id_num)\n",
    "\n",
    "    save_img_folder = os.path.join(main_path, 'unchecked/nii_img', case_id)\n",
    "    save_seg_folder = os.path.join(main_path, 'unchecked/nii_manual_seg', case_id)\n",
    "\n",
    "    if os.path.isfile(os.path.join(main_path, 'nii_manual_seg',case_id, 'SAX_ED_seg.nii.gz' )) == 1:\n",
    "        print('have checked segmentation, continue')\n",
    "        continue\n",
    "\n",
    "    # find the ED \n",
    "    full_list = pd.read_excel(os.path.join(main_path, 'Patient_list', 'full_list.xlsx'))\n",
    "    ed = int(full_list.loc[full_list['OurID'] == case_id_num]['ED'])\n",
    "    es = int(full_list.loc[full_list['OurID'] == case_id_num]['ES'])\n",
    "    \n",
    "    # load the ED image\n",
    "    img_file = os.path.join(main_path, 'nrrd', 'need_' + case_id, 'Org3D_frame'+str(ed) +'.nrrd')\n",
    "    img_file = sitk.ReadImage(img_file)\n",
    "    img = sitk.GetArrayFromImage(img_file)\n",
    "    spacing = img_file.GetSpacing()\n",
    "    print(spacing)\n",
    "    \n",
    "    img = np.rollaxis(sitk.GetArrayFromImage(img_file),0,3)\n",
    "\n",
    "    # load contour points and turn into masks\n",
    "    # contour folder\n",
    "    contour_folder = os.path.join(main_path, 'unchecked/contours',  case_id, 'manual_contours')\n",
    "    endo_files = ff.sort_timeframe(ff.find_all_target_files(['Endo*'],contour_folder),2,'_')\n",
    "    epi_files = ff.sort_timeframe(ff.find_all_target_files(['Epi*'],contour_folder),2,'_')\n",
    "    assert len(endo_files) == len(epi_files)\n",
    "\n",
    "    # convert to mask\n",
    "    seg = np.zeros(img.shape)\n",
    "\n",
    "    for file_i in range(0,len(endo_files)):\n",
    "        slice_index = ff.find_timeframe(endo_files[file_i], 2, '_')\n",
    "            \n",
    "        endo_pts = []; epi_pts = []\n",
    "\n",
    "        # get endo and epi contour points\n",
    "        with open(endo_files[file_i], 'r') as f:\n",
    "            endo_data = json.load(f)\n",
    "            endo_data = endo_data['markups'][0]['controlPoints']\n",
    "        for l in range(0,len(endo_data)):\n",
    "            # if it's manual drawing from 3D slicer, need to convert from mm to pixel unit\n",
    "            endo_pts.append([endo_data[l]['position'][0] / spacing[0], endo_data[l]['position'][1] / spacing[1]])\n",
    "\n",
    "        with open(epi_files[file_i], 'r') as f:\n",
    "            epi_data = json.load(f)\n",
    "        epi_data = epi_data['markups'][0]['controlPoints']\n",
    "        for l in range(0,len(epi_data)):\n",
    "            # if it's manual drawing from 3D slicer, need to convert from mm to pixel unit\n",
    "            epi_pts.append([epi_data[l]['position'][0] / spacing[0], epi_data[l]['position'][1] / spacing[1]])\n",
    "\n",
    "        cts_dict = {\"Endo\":np.asarray(endo_pts).astype(int), \"Epi\": np.asarray(epi_pts).astype(int), \"RV\": None}\n",
    "\n",
    "        seg[:,:,slice_index] = util.contourpts_to_mask(cts_dict, np.zeros([img.shape[0], img.shape[1]]),  ['LV', 'Myo'], [1,2], sample_rate = 1)\n",
    "\n",
    "    ff.make_folder([save_img_folder, save_seg_folder])\n",
    "    \n",
    "    img = ff.nrrd_to_nii_orientation(np.rollaxis(img,2,0))\n",
    "    seg = ff.nrrd_to_nii_orientation(np.rollaxis(seg,2,0))\n",
    "\n",
    "\n",
    "    # borrow the affine and header\n",
    "    f = nb.load(os.path.join(main_path, 'nii_img' , 'ID_0015', 'Org3D_frame1.nii.gz'))\n",
    "    affine_matrix = f.affine\n",
    "    header = f.header\n",
    "\n",
    "    # borrowed x and y dim:\n",
    "    header['dim'][1:3] = img.shape[1:3]\n",
    "    # scale:\n",
    "    scale = [spacing[0] / header['pixdim'][1],  spacing[1] / header['pixdim'][2],1]\n",
    "    S = np.diag([scale[0], scale[1], scale[2], 1])\n",
    "    \n",
    "    new_affine = np.matmul(affine_matrix, S)\n",
    "    affine_matrix = np.copy(new_affine)\n",
    "\n",
    "    nb.save(nb.Nifti1Image(img,affine=affine_matrix, header = header), os.path.join(save_img_folder, 'Org3D_frame'+str(ed) +'.nii.gz'))\n",
    "    nb.save(nb.Nifti1Image(seg, affine = affine_matrix, header = header), os.path.join(save_seg_folder, 'SAX_ED_seg.nii.gz'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
