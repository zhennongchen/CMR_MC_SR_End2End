{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this script includes: \n",
    "\n",
    "(1) turn the manual contours from control points to masks\n",
    "\n",
    "(2) only keep the slices that have segmentation (LV + myo)\n",
    "\n",
    "(3) n4 bias correction\n",
    "\n",
    "(4) min-max normalization\n",
    "\n",
    "(5) keep the voxel dimension uniform [1.25, 1.25, 2]\n",
    "\n",
    "(6) make ROI image for LV and myocardium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CMR_HFpEF_Analysis.Defaults as Defaults\n",
    "import CMR_HFpEF_Analysis.functions_collection as ff\n",
    "import CMR_HFpEF_Analysis.Image_utils as util\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import SimpleITK as sitk\n",
    "# import cv2\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "\n",
    "main_path = '/mnt/mount_zc_NAS/HFpEF/data/HFpEF_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Turn control points (in json file) into a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ID_0011\n",
      "done, continue\n",
      "1 ID_0128\n",
      "done, continue\n",
      "2 ID_0131\n",
      "done, continue\n",
      "3 ID_0151\n",
      "done, continue\n",
      "4 ID_0175\n",
      "done, continue\n",
      "5 ID_0198\n",
      "done, continue\n",
      "6 ID_0212\n",
      "done, continue\n",
      "7 ID_0227\n",
      "done, continue\n",
      "8 ID_0252\n",
      "done, continue\n",
      "9 ID_0258\n",
      "done, continue\n",
      "10 ID_0267\n",
      "1\n",
      "(1.3542, 1.3542, 8.0)\n",
      "11 ID_0331\n",
      "done, continue\n",
      "12 ID_0354\n",
      "done, continue\n",
      "13 ID_0360\n",
      "done, continue\n",
      "14 ID_0406\n",
      "done, continue\n",
      "15 ID_0417\n",
      "done, continue\n",
      "16 ID_0540\n",
      "done, continue\n",
      "17 ID_0550\n",
      "done, continue\n",
      "18 ID_0554\n",
      "done, continue\n",
      "19 ID_0566\n",
      "done, continue\n",
      "20 ID_0571\n",
      "1\n",
      "(1.3281, 1.3281, 8.0)\n",
      "21 ID_0576\n",
      "1\n",
      "(1.4583, 1.4583, 8.0)\n",
      "22 ID_0581\n",
      "1\n",
      "(0.625, 0.625, 8.0)\n",
      "23 ID_0585\n",
      "1\n",
      "(1.3281, 1.3281, 8.0)\n",
      "24 ID_0586\n",
      "1\n",
      "(1.0938, 1.0938, 8.0)\n",
      "25 ID_0590\n",
      "1\n",
      "(1.0938, 1.0938, 8.0)\n",
      "26 ID_0635\n",
      "done, continue\n",
      "27 ID_0636\n",
      "done, continue\n",
      "28 ID_0638\n",
      "done, continue\n",
      "29 ID_0709\n",
      "done, continue\n",
      "30 ID_0718\n",
      "done, continue\n",
      "31 ID_0730\n",
      "done, continue\n",
      "32 ID_0762\n",
      "done, continue\n",
      "33 ID_0865\n",
      "done, continue\n",
      "34 ID_0871\n",
      "7\n",
      "(1.0938, 1.0938, 8.0)\n",
      "35 ID_0915\n",
      "done, continue\n",
      "36 ID_0969\n",
      "1\n",
      "(1.3281, 1.3281, 8.0)\n",
      "37 ID_0977\n",
      "done, continue\n",
      "38 ID_0980\n",
      "done, continue\n",
      "39 ID_0984\n",
      "done, continue\n",
      "40 ID_1013\n",
      "done, continue\n",
      "41 ID_1037\n",
      "done, continue\n",
      "42 ID_1054\n",
      "done, continue\n",
      "43 ID_1055\n",
      "1\n",
      "(1.25, 1.25, 8.0)\n",
      "44 ID_1075\n",
      "done, continue\n",
      "45 ID_1077\n",
      "done, continue\n",
      "46 ID_1286\n",
      "done, continue\n",
      "47 ID_1321\n",
      "done, continue\n",
      "48 ID_1344\n",
      "1\n",
      "(1.3281, 1.3281, 8.0)\n",
      "49 ID_1388\n",
      "1\n",
      "(1.3281, 1.3281, 8.0)\n"
     ]
    }
   ],
   "source": [
    "# main script\n",
    "case_list = ff.find_all_target_files(['ID_*'], os.path.join(main_path, 'contours'))\n",
    "\n",
    "for i in range(0,len(case_list)):\n",
    "    case = case_list[i]\n",
    "    case_id = os.path.basename(case)\n",
    "    print(i, case_id)\n",
    "\n",
    "    save_folder = os.path.join(main_path, 'masks', case_id)\n",
    "    if os.path.isfile(os.path.join(save_folder, 'mask_myo.nii.gz')) == 1:\n",
    "        print('done, continue')\n",
    "        continue\n",
    "\n",
    "    # find the ED \n",
    "    edes_file = os.path.join(main_path, 'ED_ES', case_id,'ED_ES.txt')\n",
    "    # prepare a blank txt:\n",
    "    with open(edes_file, 'r') as file:\n",
    "        line = file.readline().strip()\n",
    "        numbers = line.split()\n",
    "        ed = int(numbers[0])\n",
    "    print(ed)\n",
    "    # load the ED image\n",
    "    img_file = os.path.join(main_path, 'raw_img', case_id, 'Org3D_frame'+str(ed) +'.nrrd')\n",
    "    img_file = sitk.ReadImage(img_file)\n",
    "    spacing = img_file.GetSpacing()\n",
    "    print(spacing)\n",
    "    img = np.rollaxis(sitk.GetArrayFromImage(img_file),0,3)\n",
    "\n",
    "    # load contour points and turn into masks\n",
    "    # contour folder\n",
    "    contour_folder = os.path.join(main_path, 'contours',  case_id, 'manual_contours')\n",
    "    endo_files = ff.sort_timeframe(ff.find_all_target_files(['Endo*'],contour_folder),2,'_')\n",
    "    epi_files = ff.sort_timeframe(ff.find_all_target_files(['Epi*'],contour_folder),2,'_')\n",
    "    assert len(endo_files) == len(epi_files)\n",
    "\n",
    "    # convert to mask\n",
    "    seg = np.zeros(img.shape)\n",
    "\n",
    "    for file_i in range(0,len(endo_files)):\n",
    "        slice_index = ff.find_timeframe(endo_files[file_i], 2, '_')\n",
    "            \n",
    "        endo_pts = []; epi_pts = []\n",
    "\n",
    "        # get endo and epi contour points\n",
    "        with open(endo_files[file_i], 'r') as f:\n",
    "            endo_data = json.load(f)\n",
    "            endo_data = endo_data['markups'][0]['controlPoints']\n",
    "        for l in range(0,len(endo_data)):\n",
    "            # if it's manual drawing from 3D slicer, need to convert from mm to pixel unit\n",
    "            endo_pts.append([endo_data[l]['position'][0] / spacing[0], endo_data[l]['position'][1] / spacing[1]])\n",
    "\n",
    "        with open(epi_files[file_i], 'r') as f:\n",
    "            epi_data = json.load(f)\n",
    "        epi_data = epi_data['markups'][0]['controlPoints']\n",
    "        for l in range(0,len(epi_data)):\n",
    "            # if it's manual drawing from 3D slicer, need to convert from mm to pixel unit\n",
    "            epi_pts.append([epi_data[l]['position'][0] / spacing[0], epi_data[l]['position'][1] / spacing[1]])\n",
    "\n",
    "        cts_dict = {\"Endo\":np.asarray(endo_pts).astype(int), \"Epi\": np.asarray(epi_pts).astype(int), \"RV\": None}\n",
    "\n",
    "        seg[:,:,slice_index] = util.contourpts_to_mask(cts_dict, np.zeros([img.shape[0], img.shape[1]]),  ['LV', 'Myo'], [1,2], sample_rate = 1)\n",
    "    \n",
    "    # do some image processing\n",
    "    # only keep the slices that have segmentation\n",
    "    non_zero_slices = [z for z in range(seg.shape[2]) if np.sum(seg[:, :, z]) != 0]\n",
    "    img_heart = img[:,:, non_zero_slices]\n",
    "    seg_heart = seg[:,:,non_zero_slices]\n",
    "\n",
    "    # n4 bias correction\n",
    "    I = sitk.GetImageFromArray(img_heart)\n",
    "    I = sitk.Cast(I, sitk.sitkFloat32)\n",
    "    I = sitk.N4BiasFieldCorrection(I)\n",
    "    img_bias_corrected = sitk.GetArrayFromImage(I)\n",
    "\n",
    "    # make the min_max normalization into [0,255]\n",
    "    img_norm = ((img_bias_corrected - np.min(img_bias_corrected)) / (np.max(img_bias_corrected) - np.min(img_bias_corrected))) * 255\n",
    "\n",
    "    # make the voxel dimension uniform [1.25, 1.25, 2.0]\n",
    "    new_voxel_dim = [1.25,1.25,2]\n",
    "\n",
    "    zoom_factors = [original_dim / new_dim for new_dim, original_dim in zip(new_voxel_dim, spacing)]\n",
    "    img_zoom = zoom(img_norm, zoom_factors, order = 3)  # Order=3 for cubic intenrpolation\n",
    "    img_zoom = np.round(img_zoom)\n",
    "    seg_zoom = zoom(seg_heart, zoom_factors, order = 0) # Order = 0 for nearest neighbour\n",
    "    seg_zoom = np.round(seg_zoom)\n",
    "\n",
    "    seg_lv_zoom = np.copy(seg_zoom); seg_lv_zoom[seg_lv_zoom != 1] = 0\n",
    "    seg_myo_zoom = np.copy(seg_zoom); seg_myo_zoom[seg_myo_zoom != 2] = 0; seg_myo_zoom[seg_myo_zoom == 2] = 1\n",
    "\n",
    "    ff.make_folder([save_folder])\n",
    "    ff.save_itk(np.rollaxis(img_zoom,2,0), os.path.join(save_folder, 'img.nii.gz'), img_file, new_voxel_dim, np.eye(4))\n",
    "    ff.save_itk(np.rollaxis(seg_zoom,2,0), os.path.join(save_folder, 'mask.nii.gz'), img_file, new_voxel_dim, np.eye(4))\n",
    "    ff.save_itk(np.rollaxis(seg_lv_zoom,2,0), os.path.join(save_folder, 'mask_lv.nii.gz'), img_file, new_voxel_dim, np.eye(4))\n",
    "    ff.save_itk(np.rollaxis(seg_myo_zoom,2,0), os.path.join(save_folder, 'mask_myo.nii.gz'), img_file, new_voxel_dim, np.eye(4))\n",
    "\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
